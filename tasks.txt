My PLAN:

---

scripts to
coverate.txt and coverage-2gram.txt


0. less that 3 words.
1. load many more words.
2. test again.
3. measure time
4. measure memory
    4.1 repeat with different sizes.
5. find weights.
6. repeat re transform in R.
    all lowercase.
    ignore numbers and other chars.
    do not ignore 's.
7. app
8. presentation

optional in app:
        if expletives -> symbols
        if fullstop -> capital
        if Names -> capital
        via dictionary.


    count symbols
        3106 hashtags    
    count numbers
        grep "[[:digit:]*] .*[^[:alpha:]]" freq.1.all.txt | grep -v "'" |awk 'BEGIN { ac=0; } { ac+=$1; } END { print ac; }'
        16947
        grep "[[:digit:]*] .*[^A-Za-z]" freq.1.all.txt | grep -v "'" |awk 'BEGIN { ac=0; } { ac+=$1; } END { print ac; }'
        30878
        filtered (caf√©)


line-number, accum / tot <-- coverage.txt
awk 'BEGIN { acum=0;t=50615233; } { acum+=$1; print NR, acum/t; }' freq.1.all.txt | sed 's/,/./' >  coverage.txt



2. FREQ OF 2-NGRAM
50615230 total.
awk 'BEGIN { acum=0;t=50615230; } { acum+=$1; print NR, acum/t; }' freq.2.all.txt | sed 's/,/./' > coverage-2gram.txt
idem coverage 2-gram

