My PLAN:

---

+ publish
+ submit.
+ pretty charts.
+ spell check
explicar Freq y Position en charts.
proof reading.
double check.

---



+1. raw files.
    words in lower.
    count lines, words per file.
    unique words.
    symbols.
    numbers.
    unicode?
    split files test-case.

4. summaries.

5. plan.
    
    all lowercase.
    ignore numbers and other chars.
    do not ignore 's.
    don't reset n-grams on full stops or newlines.
    on app:
        if expletives -> symbols
        if fullstop -> capital
        if Names -> capital
        via dictionary.





 100274519 n1.blogs.txt
 97645292 n1.news.txt
 77755789 n1.twitter.txt

  899288 en_US.blogs.txt
 1010242 en_US.news.txt
 2360148 en_US.twitter.txt

words in lower.
    count lines, words per file.

4269678 en_US.all.txt
  899288 en_US.blogs.txt
 1010242 en_US.news.txt
    
 18735811 n1.blogs.txt words.
 17035050 n1.news.txt
 14844372 n1.twitter.txt

 50615233 total with symbols.
   423633 freq.1.all.txt
   230302 1 sample.

    unique words.
    symbols.
    numbers.


    count symbols
        3106 hashtags    
    count numbers
        grep "[[:digit:]*] .*[^[:alpha:]]" freq.1.all.txt | grep -v "'" |awk 'BEGIN { ac=0; } { ac+=$1; } END { print ac; }'
        16947
        grep "[[:digit:]*] .*[^A-Za-z]" freq.1.all.txt | grep -v "'" |awk 'BEGIN { ac=0; } { ac+=$1; } END { print ac; }'
        30878
        filtered (caf√©)

line-number, accum / tot <-- coverage.txt
awk 'BEGIN { acum=0;t=50615233; } { acum+=$1; print NR, acum/t; }' freq.1.all.txt | sed 's/,/./' >  coverage.txt



2. FREQ OF 2-NGRAM
50615230 total.
awk 'BEGIN { acum=0;t=50615230; } { acum+=$1; print NR, acum/t; }' freq.2.all.txt | sed 's/,/./' > coverage-2gram.txt
idem coverage 2-gram



    don't reset n-grams on full stops or newlines.

4-gram backoff. coverage depending on available memory.

5. INCREASE COVERAGE.
    via 

map words to integer values to compress usage. (calculate how much)

histograms 4-gram 3-gram 2-gram
cumulatives... logarithm.

    4-gram 3-gram 2-gram
    tables
    charts

XXXX SIZE: Shakespeare 884,421 Ours: 50,615,233 - Shakespeare unique 28,829 - Ours: 423,633 (ref http://www.opensourceshakespeare.org/stats/)







TASK 0

Tasks to accomplish

Tokenization - identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.
Profanity filtering - removing profanity and other words you do not want to predict.

Questions to consider

1 How should you handle punctuation?
2 The data contains lots of times, dates, numbers and currency values. How to handle these? Are they useful for prediction?
3 How do you find typos in the data?
4 How do you identify garbage, or the wrong language?
5 How do you define profanity? How do you ensure you don't remove words you want to include?
6 How do you handle capital and lower cases?
7 What is the best set of features you might use to predict the next word?

1 only punctuation is appostrophe (contractions)
2 avoid other thing than words. problematic and difficult to predict.
3 wrong language (do nothing)
4 no-profanity filtering. left up-to the last phase of text-prediction. (ie. replace profanity with "!@%#&!")
5 typos. forget.
6 low-up cases. all lower. left up-to the last phase. (ie. after a '.' capital)
7 relay on n-grams.


TASK 1

Tasks to accomplish

Exploratory analysis - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora.
Understand frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of words and word pairs in the data.

Questions to consider

Some words are more frequent than others - what are the distributions of word frequencies?
What are the frequencies of 2-grams and 3-grams in the dataset?
How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?
How do you evaluate how many of the words come from foreign languages?
Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?

1. DO FREQ STUDIES OF WORDS.
2. FREQ OF 2-NGRAM
3. EVALUATE WORDS ON FOREIGN LANG
4. WORDS IN A CORPUS. coverage with dictionary.
5. INCREASE COVERAGE.


Tips and hints

Consider how you are going to perform your basic exploratory and predictive analyses. Keep in mind that the first n rows of the data set may not be representative. You might want to think about how to sample the data using file() and readLines() to obtain a representative sample. 
Think hard about ways you can "compress" the data, what words appear frequently? What combinations of words appear frequently? Later, when you build the R model object, it will need to be small enough to upload to a Shiny server. The more you can figure out how to compress the data the smaller this object will be.



TASK 2

Tasks to accomplish

Build basic n-gram model - using the exploratory analysis you performed, build a basic n-gram model (http://en.wikipedia.org/wiki/N-gram) for predicting the next word based on the previous 1, 2, or 3 words.
Build a model to handle unseen n-grams - in some cases people will want to type a combination of words that does not appear in the corpora. Build a model to handle cases where a particular n-gram isn't observed.
Questions to consider

How can you efficiently store an n-gram model (think Markov Chains)?
How can you use the knowledge about word frequencies to make your model smaller and more efficient?
How many parameters do you need (i.e. how big is n in your n-gram model)?
Can you think of simple ways to "smooth" the probabilities (think about giving all n-grams a non-zero probability even if they aren't observed in the data) ?
How do you evaluate whether your model is any good?
How can you use backoff models to estimate the probability of unobserved n-grams?

Tips and hints

Think about starting simply, using your sampled data - what would be your first choice word prediction if you only knew the frequency of each individual word? 
How would you build the model if you were predicting using only one previous word? 





MILESTONE

Please upload the URL of an R Pubs document describing your exploratory analysis (http://rpubs.com/, be sure that the url that you submit is http and not https). 

Does the link lead to an HTML page describing the exploratory analysis of the training data set?
1: yes, the link does not lead to a document describing the exploratory analysis

Has the data scientist done basic summaries of the three files?
    Word counts,
    line counts and
    basic data tables?
1: yes, the data scientist has evaluated basic summaries of the data such as word and line counts

Has the data scientist made basic plots,
    such as histograms to illustrate features of the data?
1: yes, the data scientist has made basic plots, such as histograms to illustrate features of the data

Was the report written in a brief, concise style, in a way that a non-data scientist manager could appreciate?
3: yes, the report could be understood by a non data scientist and is brief and concise

