TASK 0

Tasks to accomplish

Tokenization - identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.
Profanity filtering - removing profanity and other words you do not want to predict.

Questions to consider

How should you handle punctuation?
The data contains lots of times, dates, numbers and currency values. How to handle these? Are they useful for prediction?
How do you find typos in the data?
How do you identify garbage, or the wrong language?
How do you define profanity? How do you ensure you don't remove words you want to include?
How do you handle capital and lower cases?
What is the best set of features you might use to predict the next word?


no-profanity filtering. left up-to the last phase of text-prediction. (ie. replace profanity with "!@%#&!")
typos. forget.
low-up cases. all lower. left up-to the last phase. (ie. after a '.' capital)


TASK 1

Tasks to accomplish

Exploratory analysis - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora.
Understand frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of words and word pairs in the data.

Questions to consider

Some words are more frequent than others - what are the distributions of word frequencies?
What are the frequencies of 2-grams and 3-grams in the dataset?
How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?
How do you evaluate how many of the words come from foreign languages?
Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?

Tips and hints

Consider how you are going to perform your basic exploratory and predictive analyses. Keep in mind that the first n rows of the data set may not be representative. You might want to think about how to sample the data using file() and readLines() to obtain a representative sample. 
Think hard about ways you can "compress" the data, what words appear frequently? What combinations of words appear frequently? Later, when you build the R model object, it will need to be small enough to upload to a Shiny server. The more you can figure out how to compress the data the smaller this object will be.



TASK 2

Tasks to accomplish

Build basic n-gram model - using the exploratory analysis you performed, build a basic n-gram model (http://en.wikipedia.org/wiki/N-gram) for predicting the next word based on the previous 1, 2, or 3 words.
Build a model to handle unseen n-grams - in some cases people will want to type a combination of words that does not appear in the corpora. Build a model to handle cases where a particular n-gram isn't observed.
Questions to consider

How can you efficiently store an n-gram model (think Markov Chains)?
How can you use the knowledge about word frequencies to make your model smaller and more efficient?
How many parameters do you need (i.e. how big is n in your n-gram model)?
Can you think of simple ways to "smooth" the probabilities (think about giving all n-grams a non-zero probability even if they aren't observed in the data) ?
How do you evaluate whether your model is any good?
How can you use backoff models to estimate the probability of unobserved n-grams?

Tips and hints

Think about starting simply, using your sampled data - what would be your first choice word prediction if you only knew the frequency of each individual word? 
How would you build the model if you were predicting using only one previous word? 

