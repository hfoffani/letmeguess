---
title: "Text Prediction"
author: "herchu"
date: "Sunday, March 29th. 2015"
output: html_document
---

```{r, readcsv, cache=T, echo=F}
set.seed(1234)
commonwords <- read.csv('data/freq.1.all.txt', sep=" ", stringsAsFactors=F, nrows=1000, header=F, col.names=c('Freq','Word'))
covwords <- read.csv('data/coverage-words.txt', sep=" ", header=F, col.names=c('Position','Freq'))
covwords <- covwords[sample(1:nrow(covwords), 10000, replace=FALSE),]
cov2grams <- read.csv('data/coverage-2gram.txt', sep=" ", header=F, col.names=c('Position','Freq'))
cov2grams <- cov2grams[sample(1:nrow(cov2grams), 10000, replace=FALSE),]
```

### Introduction

In spite of miniaturization, computarization and technology achivements ocurred along the last decades typing text in keyboards remains pretty similar what was a century ago. In portable digital devices like smartphones where the keyboard is extremely small the task became uncomfortable and slow. The popularization of these gadgets pressure the industry to find easier ways of entering text. Predicting the next word a user wants to type with a certain success can reduce the typing significantly.

In this report I describe a proof of concept of such technique that will be implemented as a web application, the algorithm that will drive this application, its contraints and how to measure its effectivenenss.


### The Corpus

How to predict a user intention? Given that reading the mind is out of the question (at least for time being) we must relay on past events. Analyzing the text a user have just typed and based on that predict her next word can be a reliable way. The key word here is *analyzing*. Natural Language Processing is the science branch that study the digital processing of human languages. and for this application I focus on just one technique that have proven a significant rate of success.

Succintly the idea is to take a giant database of text, split it in small short phrases, sort them by popularity (frequency of occurrence), and store them in a local database. When a user types a sentence the application takes this partial phrase and looks for the best match it the database, if it can find one it suggest the user the rest of the phrase.

This giant database of text is called the *corpus*. For this exercise the corpus consists of three files of English texts. The first of the three includes the content of web blogs, the second contains news and the third are tweets.

This files were provided by Coursera as part of Data Science Capstone project.

Internally these corpus were built with one *item* per line. I.e. one blog article per line in the blogs file, one news article per line in the news file and one tweet per line in the tweets file.

|     File             |  Number of lines  |
|:--------------------:|:-----------------:|
|                      |                   |
|    en_US.blogs.txt   |      899,288      |
|    en_US.news.txt    |    1,010,242      |
|   en_US.twitter.txt  |    2,360,148      |

The first thing I did was to split this files in train and test sets. The train set is 50% of the corpus. The rest will be used during the validation and test phase.


### Corpus Description

The rest of the study was done over the training set and to fairly compare these results with other studies the statistics will be reported in relative not absolute values.

During the preprocessing phase I translate all characters to lowercase and split them in words using regular expressions.

Due to the way these files are organized counting words per line differs significantly among the three. All in all they all share the same rate of words per file size. It can be interpreted as a density: lexical words per bytes.

```{r pander0, echo=F}
library(pander)
panderOptions("table.split.table", 1000)
Words.per.bytes <- c(
18735811/100274519, # blogs
17035050/97645292,  # news
14844372/77755789  # tweets
)
File <- c('en_US.blogs.txt','en_US.news.txt','en_US.twitter.txt')
df <- data.frame(File, Words.per.bytes)
pander(df)
```

The inverse of these values *does not* measure the mean of characters per English word, as the total byte values contains punctuation, numbers, symbols, etc.

There are some interesting facts within this corpus.

1. It contains `r 3106/423633*100`% unique hashtags.
2. There are `r 30878/423633*100`% different words with non English characters,
3. where `r 16947/423633*100`% are non-alphabetic distinct lemmas.

Counting the occurrence of words and taking the most common ones shows the expected results.

```{r pander, echo=F}
library(pander)
panderOptions("table.split.table", 1000)
out <- matrix(head(commonwords$Word, n=100), ncol=10)
set.caption("Table with the 100 most common words")
pander(out)
```


However the lemma *t* surprisingly appears within the most popular words. In this case this was due to the fact that its ocurrence adds in many different meanings: t as tablespoon acronym in receipts, t in contractions with missing apostrophe, t as "the" and "to" abreviations.

Another finding is that the word "obama" has an occurrence similar to other common words like "welcome". This suggests that the corpus for this kind of applications must be a living entity.

Misspellings and foreign lemmas also show some commonality. "adios" and "tomorow" have the same frequency as the beauty but uncommon word "triangular".

As you see foreing words were not filtered out. If they appear in an English corpus it would be for a reason.

While frequency ocurrences are commonly shown using histograms, in this case I prefer to graph them using cummulative frequencies relative to the total number of words.

```{r, echo=F}
plot(covwords)
```

It is more readably by graphing the x axis using a logarithm scale.

```{r, echo=F}
plot(covwords, log="x")
```

A layman will be surprised by the linearity of the relationship in this chart. The x axis is in a log scale which implies that the relation exists and is exponential. As a matter of fact this a well known behaviour called the Zipf's law, see <http://en.wikipedia.org/wiki/Zipf%27s_law>

While dictionary of 1,000 words would cover 70% of usage, an 80% would require to double its size to 2,000 words aproximately.

This property is key to help us to decide the size of the dictionary in the application that better fits the restricted environment.

If instead of one word we take unique pair of words and measure their frequencies we build what is called bigrams or 2-grams. Plotting them shows a similar pattern although it takes much number of them to obtain the same increase in coverage.

```{r, echo=F}
plot(cov2grams, log="x")
```


### The application

The proof of concept will be implemented as a very simple web application consisting on a text box data entry and two small information panels, one on top and one at the bottom of the text box. The user will type whatever she wants in the text box and the predicted word will appear in the top panel. If she press tab the word will be copied inside the box avoiding the need to type it.

The bottom panel will display some statistics regarding the performance of the application like the numbers of key strokes that were saved for the user, the successful and failure prediction rates.


### The engine inside the application

1. The application will ignore all punctuation symbols except for the appostrosphe.
2. Any other things excepts words like numbers will be ignored.
3. There will no special treatment for foreing languages. If some terms proves to be popular it will be included.
4. Same for typos.
5. All words will be in lower case. (see 7)
6. Profanities won't be filered out but,
7. A dictionary will translate proper names to uppercase and profanities with special characters.
8. After a stop the next suggested word will be capitalized.

Internally the application will hold a n-gram dictionary (n-1 words being the predictors, the nth the result) for n from 2 to 4.

If there is no match in the 4-gram the algorithm will fallback the 3-gram and try to match there and so on.

Increasing the coverage while mantaining the memory footprint limited can be done by keeping the internal structures using hashes instead of literal strings.

