---
title: "Text Prediction"
author: "herchu"
date: "21 de marzo de 2015"
output: html_document
---

#### TO DO

HOY:
    + count symbols
    + count numbers
    + mostfreq.table.1
    + coverage.curve.1
    + cov2gram.curve.2
    2gram.histogrm.1
    
    + insert R-code in rmd. to charts.
    add titles X/Y labels scales X/Y

TOMORROW:
    write the text
    
### Introduction

In spite of miniaturization, computarization and technology achivements ocurred along the last decades typing text in keyboards remains pretty similar what was a century ago. In portable digital devices like smartphones where the keyboard is extremely small the task became uncomfortable and slow. The popularization of these gadgets pressure the industry to find easier ways of entering text. Predicting the next word a user wants to type with a certain success can reduce the typing significantly.

In this report I describe a proof of concept of such technique that will be implemented as a web application, the algorithm that will drive this application, its contraints and how to measure its effectivenenss.


### The Corpus

How to predict a user intention? Given that reading the mind is out of the question (at least for time being) we must relay on past events. 


    unicode?
    split files test-case. X%


### Describe the Corpus

    words in lower.
    count lines, words per file.
    
 18735811 n1.blogs.txt words.
 17035050 n1.news.txt
 14844372 n1.twitter.txt

 50615233 total with symbols.
   423633 freq.1.all.txt
   230302 1 sample.

    unique words.
    symbols.
    numbers.

    count symbols
        3106 hashtags    
    count numbers
        grep "[[:digit:]*] .*[^[:alpha:]]" freq.1.all.txt | grep -v "'" |awk 'BEGIN { ac=0; } { ac+=$1; } END { print ac; }'
        16947
        grep "[[:digit:]*] .*[^A-Za-z]" freq.1.all.txt | grep -v "'" |awk 'BEGIN { ac=0; } { ac+=$1; } END { print ac; }'
        30878
        filtered (caf√©)


```{r, cache=T}
commonwords <- read.csv('data/freq.1.all.txt', sep=" ", stringsAsFactors=F, nrows=1000, header=F, col.names=c('Freq','Word'))
covwords <- read.csv('data/coverage-words.txt', sep=" ", header=F, col.names=c('Position','Freq'))
cov2grams <- read.csv('data/coverage-2gram.txt', sep=" ", header=F, col.names=c('Position','Freq'))
set.seed(1234)
covwords <- covwords[sample(1:nrow(covwords), 10000, replace=FALSE),]
cov2grams <- cov2grams[sample(1:nrow(cov2grams), 10000, replace=FALSE),]
```

1. DO FREQ STUDIES OF WORDS.

most frequent words.

```{r pander}
library(pander)
panderOptions("table.split.table",1000)
out <- matrix(head(commonwords$Word, n=100), ncol=10)
set.caption("Most common words")
pander(out)
```

=== mostfreq.table.1
obama == welcome
t teaspoon the to (..n't contraction missing ')

adios == triangular == tomorow
hashtags.  #20thingsaboutme

3. EVALUATE WORDS ON FOREIGN LANG
nada.

line-number, accum / tot <-- coverage.txt
awk 'BEGIN { acum=0;t=50615233; } { acum+=$1; print NR, acum/t; }' freq.1.all.txt | sed 's/,/./' >  coverage.txt
=== coverage.curve.1
```{r}
plot(covwords)
plot(covwords, log="x")
```

4. WORDS IN A CORPUS. coverage with dictionary.


### Building the data structures

1 only punctuation is appostrophe (contractions)
2 avoid other thing than words. problematic and difficult to predict.
3 wrong language (do nothing)
4 no-profanity filtering. left up-to the last phase of text-prediction. (ie. replace profanity with "!@%#&!")
5 typos. forget.
6 low-up cases. all lower. left up-to the last phase. (ie. after a '.' capital)

    don't reset n-grams on full stops or newlines.

2. FREQ OF 2-NGRAM
50615230 total.
awk 'BEGIN { acum=0;t=50615230; } { acum+=$1; print NR, acum/t; }' freq.2.all.txt | sed 's/,/./' > coverage-2gram.txt
idem coverage 2-gram
cov2gram.curve.2
```{r}
plot(cov2grams, log="x")
```


histograms 4-gram 3-gram 2-gram
cumulatives... logarithm.

    4-gram 3-gram 2-gram
    tables
    charts


### Describe the app

user interface.


### Describe the algorithm

7 relay on n-grams.

4-gram backoff. coverage depending on available memory.

5. INCREASE COVERAGE.
    via 

map words to integer values to compress usage. (calculate how much)

